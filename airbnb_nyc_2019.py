# -*- coding: utf-8 -*-
"""airbnb_nyc_2019.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cce8bp2vkXFKdfxX_g5fHOT65qJt6oBW

Data Preprocessing

In general, a dataset that comes directly from a source is raw, for i.e. inconsistencies, misspelled values, NA values, etc.

In such cases, it is a must to apply data preprocessing before training the model in the next steps.

Methods for improving a dataset usually involve discarding some records and replacing them with meaningful values, to name a few.
"""

# importing libraries 
import pandas as pd
import numpy as np
import seaborn as sns
sns.set(rc={'figure.figsize':(10, 5)})

"""Loading and Exploring the DataFrame

Having loaded the dataset, to gain a better understanding of the values, it's of vital to obtain simple statistics regarding the columns.
"""

df = pd.read_csv("/content/AB_NYC_2019.csv") 
df.head()

df.shape

df.size

df.columns

df.info()

df.describe()

"""Dropping unused Columns

If a column will not be used during machine learning model training, it'd be useful to drop these columns.

Dropping such columns will shrink the size of the dataset and therefore will save from memory.
"""

df.drop(["name", "id", "host_id", "host_name"], inplace=True, axis=1)
df.head()

"""Outliers Detection

In most simple terms, an outlier is a data that lies quite far away from the other data points/or is not consistent with the pattern seen in the dataframe.

It can be due to entry errors, or measurement errors, or can even be a natural outlier as well.

The most common method used for detecting the outliers is visualization, and the widely-accepted principle "Outliers lie three or more standard deviation away from the mean." shows us hints on a data point being an outlier.

Outlier Detection: Z-Score

Z-score is a statistical measurement which indicates how many standard deviations away a given observation is from the mean.

Every single data point has a different Z-score, however, the dataset only has a single mean and a single standard deviation value.

Z-score = x-mean /  Standard Deviation

Imputation

Imputation is filling missing or outlier values with meaningful data.
"""

integers = ["minimum_nights", "number_of_reviews", "calculated_host_listings_count", "availability_365", "price"]

for column in integers:
  df[column] = pd.to_numeric(df[column], errors="coerce")

# finds the max value (and the min value) that is not going to be called not an outlier

  max_value =  (df[column].std()*3) +  df[column].mean()
  min_value = -((df[column].std()*3) +  df[column].mean())

# calculate the new mean of the column without the outlier values
mean = df.loc[(df[column] > min_value) & (df[column] < max_value), column].mean()

# if there is no outlier value then the mean will be np.nan in this situation don't need to do imputation
if mean is not np.nan:
  mean = df.loc[(df[column] < min_value) | (df[column] > max_value), column]  # change outlier values with mean
  df[column].fillna(mean, inplace=True)   # also fill nan values with mean

else:   # this condition means there is no outlier value in the column
  df[column].fillna(df[column].mean(), inplace=True)  # just fill nan values with mean

  df[column] = df[column].astype("int64")  # the type will be converted to int64

sns.histplot(df["price"], kde=True, bins=100)

floats = ["reviews_per_month", "latitude", "longitude"]

for column in floats:
  df[column] = pd.to_numeric(df[column], errors="coerce")

# finds the max value (and the min value) that is not going to be called not an outlier
  
  max_value =  (3*df[column].std()) + df[column].mean() 
  min_value = -((3*df[column].std()) + df[column].mean())

# calculate the new mean of the column without the outlier values
mean = df.loc[(df[column] > min_value) & (df[column] < max_value), column].mean()

# if there is no outlier value then the mean will be np.nan in this situation don't need to do imputation
if mean is not np.nan:  
    mean = df.loc[(df[column] < min_value) | (df[column] > max_value), column]  # change outlier values with mean
    df[column].fillna(mean, inplace=True)  # also fill nan values with mean

else:  # this condition means there is no outlier value in the column
    df[column].fillna(df[column].mean(), inplace=True)  # filling NA values with mean

df[column] = df[column].astype("float64") #converting the type to int64

sns.histplot(df["price"], kde=True, bins=100)

df.info()

"""Handling missing categorical values"""

# .value_counts() is used to see the values in column "room_type"
df.groupby("room_type")["room_type"].value_counts()

# prints the number of NA values for each column
df.isna().sum()

# assigning NAs of the date column to "unknown_type"
df.loc[df.isna()["last_review"],"last_review"] = "unknown_date"

df.dropna(how = 'any', inplace = True)

df.isna().sum()

"""Converting dtypes"""

df.info()

object_columns = df.select_dtypes("object").columns
object_columns

for column in object_columns:
  df[column] = df[column].astype("category")

df.info()

df.describe().T

"""Pivot Table

Creates a spreadsheet-style pivot table as a DataFrame
"""

df.pivot_table(values=["number_of_reviews","availability_365",'price'], 
               columns="room_type",
               aggfunc=np.mean)